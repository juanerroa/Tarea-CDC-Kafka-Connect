
# üìñ Change Data Capture con SQL Server ‚Üí Kafka ‚Üí PostgreSQL

## üéØ Introducci√≥n y Prop√≥sito

Este proyecto demuestra c√≥mo replicar cambios en tiempo real desde una base de datos **SQL Server** hacia **PostgreSQL** utilizando **Apache Kafka** y **Kafka Connect**.

El caso de uso principal es la replicaci√≥n de datos transaccionales, como una tabla de clientes de un sistema bancario, hacia un destino anal√≠tico o un data warehouse para su posterior procesamiento sin afectar la base de datos de producci√≥n.

### Beneficios del Enfoque

  * ‚ö° **Captura de Cambios en Tiempo Real:** Los datos se replican con baja latencia a medida que se producen.
  * üõ† **Escalabilidad y Desacoplamiento:** Los sistemas de origen y destino no est√°n directamente acoplados, lo que permite un mantenimiento y escalado independientes.
  * üîå **F√°cil Integraci√≥n:** Kafka act√∫a como un bus central de eventos, facilitando la conexi√≥n de m√∫ltiples sistemas adicionales (consumidores o productores) en el futuro.

-----

## üìÅ Estructura del Proyecto

El repositorio est√° organizado de la siguiente manera para facilitar el despliegue y la configuraci√≥n:

```
CDC-Project/
‚îú‚îÄ docker-compose.yml        # Define y orquesta todos los servicios en contenedores.
‚îú‚îÄ db.init/                  # Guarda las tablas necesarias a crear en las bases de datos
‚îÇ  ‚îú‚îÄ sqlserver.sql          # Tablas de SQL a crear para usar como fuente de datos par ala demostraci√≥n
‚îú‚îÄ connectors/
‚îÇ  ‚îú‚îÄ sqlserver-source.json  # Configuraci√≥n del conector de origen (SQL Server).
‚îÇ  ‚îî‚îÄ postgres-sink.json     # Configuraci√≥n del conector de destino (PostgreSQL).
‚îú‚îÄ init-connectors.bash      # Script para registrar los conectores en la API de Kafka Connect.
‚îú‚îÄ postman_collection.json   # Colecci√≥n de postman para gestionar los conectores (opcionalmente si no se usa Curl)
‚îî‚îÄ README.md                 # Doc
```

Se incluye un archivo `.gitignore` para excluir archivos innecesarios y sensibles del control de versiones, tales como:

  * Datos de vol√∫menes de Docker.
  * Logs generados por las aplicaciones.
  * Archivos temporales del sistema.
  * Configuraciones locales del IDE.
  * Archivos de credenciales como `.env`.

-----

## üèõÔ∏è Arquitectura T√©cnica Detallada

### Componentes del Sistema

  * üñ• **SQL Server 2022:** Base de datos relacional que act√∫a como la fuente de datos transaccionales.
  * ‚ö° **Apache Kafka:** Plataforma de streaming de eventos que funciona como bus de mensajes.
  * üîó **Kafka Connect:** Framework para conectar Kafka con sistemas externos de forma fiable y escalable.
  * üêò **PostgreSQL:** Base de datos relacional de c√≥digo abierto que act√∫a como destino de los datos.

### Flujo de Datos Paso a Paso

El flujo de datos sigue una secuencia l√≥gica y desacoplada a trav√©s de los componentes:

```
1. SQL Server (Tabla 'Clients')
        ‚îÇ
        ‚ñº
2. Kafka Connect Source Connector (JDBC)
        ‚îÇ (Lee cambios basados en una columna incremental)
        ‚ñº
3. Kafka Topic: 'SQLServer_Clients'
        ‚îÇ (Los cambios se publican como mensajes)
        ‚ñº
4. Kafka Connect Sink Connector (JDBC)
        ‚îÇ (Consume mensajes del topic)
        ‚ñº
5. PostgreSQL (Tabla 'clients_new')
```

-----

## ‚öôÔ∏è Requisitos del Sistema

### Software

  * **Docker y Docker Compose:** Versi√≥n `3.9` o superior.
  * **Puertos Disponibles:**
      * `1433` ‚Üí SQL Server
      * `5432` ‚Üí PostgreSQL
      * `9092` ‚Üí Kafka Broker
      * `8083` ‚Üí Kafka Connect REST API

### Hardware (M√≠nimo)

  * **RAM:** 4GB
  * **CPU:** 2 n√∫cleos

-----

## üöÄ Despliegue Express (5 minutos)

Sigue estos pasos para levantar toda la infraestructura y el pipeline.

1.  **Levantar todos los servicios con Docker Compose:**

    ```bash
    docker compose up -d
    ```

2.  **Verificar que todos los contenedores est√©n activos:**

    ```bash
    docker ps
    ```

3.  **Revisar los logs de Kafka Connect** para asegurar que se ha iniciado correctamente:

    ```bash
    docker logs -f tarea-1-kafka-connect-1
    ```

    *Busca un mensaje que indique que la API REST est√° escuchando en el puerto `8083`.*

-----

## üìã Configuraci√≥n de Conectores

Los conectores se definen en archivos JSON y se registran a trav√©s de la API REST de Kafka Connect.

### Source Connector ‚Äì SQL Server

Este conector lee la tabla `Clients` en modo incremental usando la columna `ClientId`.

`connectors/sqlserver-source.json`

```json
{
  "name": "sqlserver-source",
  "config": {
    "connector.class": "io.confluent.connect.jdbc.JdbcSourceConnector",
    "tasks.max": "1",
    "connection.url": "jdbc:sqlserver://sqlserver:1433;databaseName=BankingCore",
    "connection.user": "sa",
    "connection.password": "DummyPass123!",
    "mode": "incrementing",
    "incrementing.column.name": "ClientId",
    "table.whitelist": "Clients",
    "topic.prefix": "SQLServer_",
    "poll.interval.ms": 5000
  }
}
```

### Sink Connector ‚Äì PostgreSQL

Este conector consume los mensajes del topic `SQLServer_Clients` y los inserta en una nueva tabla llamada `clients_new`.

`connectors/postgres-sink.json`

```json
{
  "name": "postgres-sink",
  "config": {
    "connector.class": "io.confluent.connect.jdbc.JdbcSinkConnector",
    "tasks.max": "1",
    "connection.url": "jdbc:postgresql://postgres-sink:5432/postgres",
    "connection.user": "sa",
    "connection.password": "DummyPass123!",
    "auto.create": "true",
    "auto.evolve": "true",
    "delete.enabled": "false",
    "topics": "SQLServer_Clients",
    "table.name.format": "clients_new"
  }
}
```

### Registrar los Conectores

Ejecutar el script para enviar las configuraciones a la API de Kafka Connect:

```bash
./init-connectors.bash
```

-----

## üß™ Pruebas de Integraci√≥n

Para validar que el pipeline funciona de extremo a extremo:

1.  **Insertar datos de prueba en SQL Server:**
    Se puede usar un cliente SQL o ejecutar el siguiente comando a trav√©s de Docker:

    ```sql
    INSERT INTO Clients (FirstName, LastName, Email) VALUES ('Juan', 'Perez', 'juan.perez@example.com');
    ```

2.  **Verificar que los datos llegaron a PostgreSQL:**
    Con√©ctarse a la base de datos de PostgreSQL y ejecuta una consulta:

    ```sql
    SELECT * FROM clients_new;
    ```

    *Se deberia ver el registro de 'Juan Perez' despu√©s de unos segundos (seg√∫n el `poll.interval.ms`).*

-----

## üîç Verificaci√≥n y Monitoreo

Usar la API REST de Kafka Connect para verificar el estado de los conectores:

```bash
# Listar todos los conectores activos
curl http://localhost:8083/connectors

# Verificar el estado del conector de origen
curl http://localhost:8083/connectors/sqlserver-source/status

# Verificar el estado del conector de destino
curl http://localhost:8083/connectors/postgres-sink/status
```

Para depuraci√≥n, los logs del contenedor de Kafka Connect la principal fuente de informaci√≥n:

```bash
docker logs -f tarea-1-kafka-connect-1
```

-----

## üîß Problemas Comunes

  * **`InvalidReplicationFactorException`:** Ocurre si solo tienes un broker de Kafka. Aseg√∫rate de que las variables de entorno para los topics internos de Connect (`CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR`, etc.) est√©n configuradas en `1`.
  * **Puerto REST no accesible:** Revisa la variable `CONNECT_REST_ADVERTISED_HOST_NAME` en tu `docker-compose.yml` para asegurarte de que es accesible desde tu host.
  * **Datos no replicados:** Verifica que la columna `incrementing.column.name` existe y sus valores aumentan. Revisa tambi√©n el `poll.interval.ms` por si la latencia es mayor de la esperada.

-----

## üîí Seguridad

  * **No subir credenciales:** Nunca incluyas contrase√±as directamente en los archivos de configuraci√≥n en un repositorio p√∫blico.
  * **Usar `.env`:** Almacena credenciales y datos sensibles en un archivo `.env` y refer√©ncialo desde tu `docker-compose.yml`.
  * **Limitar acceso:** En un entorno de producci√≥n, restringe el acceso a la API REST de Kafka Connect a trav√©s de un firewall o una red privada.

-----

## ‚ö° Optimizaci√≥n y Buenas Pr√°cticas

  * **Memoria de Kafka Connect:** Ajusta la memoria de la JVM para Kafka Connect usando la variable de entorno `HEAP_OPTS` para manejar cargas de trabajo m√°s grandes.
  * **Ajuste del `poll.interval.ms`:** Un intervalo m√°s bajo reduce la latencia pero aumenta la carga en la base de datos de origen. Aj√∫stalo seg√∫n tus necesidades.
  * **Monitoreo:** Implementa herramientas de monitoreo para supervisar el rendimiento (throughput, latencia) y configurar alertas para logs de errores.
